Install TensorFlow
pip install --user --upgrade tensorflow

Clone the example repo
git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git
cd cloudml-samples/census/estimator

Get your training data
mkdir data
gsutil -m cp gs://cloud-samples-data/ml-engine/census/data/* data/

Note: Gsutil works on pip2 install only
sudo apt-get remove --purge gsutil  
sudo pip2 install gsutil  


Now set the TRAIN_DATA and EVAL_DATA variables to your local file paths 
export TRAIN_DATA=$(pwd)/data/adult.data.csv
export EVAL_DATA=$(pwd)/data/adult.test.csv

To open the adult.data.csv
head data/adult.data.csv

Install dependencies
pip install --user -r ../requirements.txt

Run a local training job
export MODEL_DIR=output

Run this training job locally by running the following command:
gcloud ai-platform local train \
    --module-name trainer.task \
    --package-path trainer/ \
    --job-dir $MODEL_DIR \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --eval-steps 100

Inspect the summary logs using Tensorboard - webased GUI for error and accuracy
Click on Accuracy to see graphical representations of how accuracy changes as your job progresses.
tensorboard --logdir=$MODEL_DIR --port=8080

The output/export/census directory holds the model exported as a result of running training locally. List that directory to see the generated timestamp subdirectory:
    ls output/export/census/

Copy the timestamp that was generatedâ€”you will use it in the following sections.
1547669983

Use your trained model for prediction
Once you've trained your TensorFlow model, you can use it for prediction on new data. In this case, you've trained a census model to predict income category given some information about a person.
For this section we'll use a predefined prediction request file, test.json, included in the GitHub repository in the census directory. You can inspect the JSON file in the Cloud Shell editor if you're interested in learning more.


Run your training job in the cloud
Set up a Google Cloud Storage bucket
The AI Platform services need to access Google Cloud Storage (GCS) to read and write data during model training and batch prediction.

First, set the following variables:

PROJECT_ID=$(gcloud config list project --format "value(core.project)")
BUCKET_NAME=${PROJECT_ID}-mlengine
echo $BUCKET_NAME
REGION=us-central1

Create the new bucket:

gsutil mb -l $REGION gs://$BUCKET_NAME

Upload the data files to your Cloud Storage bucket.
Use gsutil to copy the two files to your Cloud Storage bucket:

gsutil cp -r data gs://$BUCKET_NAME/data

Set the TRAIN_DATA and EVAL_DATA variables to point to the files:

TRAIN_DATA=gs://$BUCKET_NAME/data/adult.data.csv
EVAL_DATA=gs://$BUCKET_NAME/data/adult.test.csv

Use gsutil again to copy the JSON test file test.json to your Cloud Storage bucket:

gsutil cp ../test.json gs://$BUCKET_NAME/data/test.json

Set the TEST_JSON variable to point to that file:

TEST_JSON=gs://$BUCKET_NAME/data/test.json

Run a single-instance trainer in the cloud
With a validated training job that runs in both single-instance and distributed mode, you're now ready to run a training job in the cloud. You'll start by requesting a single-instance training job.
Use the default BASIC scale tier to run a single-instance training job. The initial job request can take a few minutes to start, but subsequent jobs run more quickly. This enables quick iteration as you develop and validate your training job.
Select a name for the initial training run that distinguishes it from any subsequent training runs. For example, you can append a number to represent the iteration:

JOB_NAME=census_single_1


Specify a directory for output generated by AI Platform by setting an OUTPUT_PATH variable to include when requesting training and prediction jobs. The OUTPUT_PATH represents the fully qualified Cloud Storage location for model checkpoints, summaries, and exports. You can use the BUCKET_NAME variable you defined in a previous step. It's a good practice to use the job name as the output directory:

OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME

Run the following command to submit a training job in the cloud that uses a single process. This time, set the --verbosity tag to DEBUG so that you can inspect the full logging output and retrieve accuracy, loss, and other metrics. The output also contains a number of other warning messages that you can ignore for the purposes of this sample:

gcloud ai-platform jobs submit training $JOB_NAME \
    --job-dir $OUTPUT_PATH \
    --runtime-version 1.10 \
    --module-name trainer.task \
    --package-path trainer/ \
    --region $REGION \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --eval-steps 100 \
    --verbosity DEBUG

You can monitor the progress of your training job by watching the logs on the command line:

gcloud ai-platform jobs stream-logs $JOB_NAME

Or monitor in the Console: AI Platform > Jobs.

Inspect the output
If you have been monitoring the job in Cloud Shell, you will know you're done when your output resembles the following:

INFO    2019-01-16 12:58:34 -0800     master-replica-0      Task completed successfully.

In cloud training, outputs are produced in Cloud Storage. In this sample, outputs are saved to OUTPUT_PATH; to list them, run:

gsutil ls -r $OUTPUT_PATH

The outputs should be similar to the outputs from training locally (above).

Deploy your model to support prediction
By deploying your trained model to AI Platform to serve online prediction requests, you get the benefit of scalable serving. This is useful if you expect your trained model to be hit with many prediction requests in a short period of time.
Wait until your AI Platform training job is done. It is finished when you see a green check mark by the jobname in the Cloud Console, or when you see the message Job completed successfully in the Cloud Shell command line.

Create an AI Platform model:

MODEL_NAME=census

Create an AI Platform model:

gcloud ai-platform models create $MODEL_NAME --regions=$REGION

Select the exported model to use, by looking up the full path of your exported trained model binaries.

gsutil ls -r $OUTPUT_PATH/export

Note:(gsutil ls -r $OUTPUT_PATH/export
In the output, you can see a number (like 1569256565). It's the new timestamp)

Scroll through the output to find the value of $OUTPUT_PATH/export/census/<timestamp>/. Copy timestamp and add it to the following command to set the environment variable MODEL_BINARIES to its value:

MODEL_BINARIES=$OUTPUT_PATH/export/census/<timestamp>/

eg:MODEL_BINARIES=$OUTPUT_PATH/export/census/1571036086/

Note: The timestamp for this training run will not be the same as the timestamp generated during your local training run above. Be sure to scroll through the gsutil ls output to find this new timestamp.

You'll deploy this trained model.

Run the following command to create a version v1 of your model:

gcloud ai-platform versions create v1 \
--model $MODEL_NAME \
--origin $MODEL_BINARIES \
--runtime-version 1.10

It may take several minutes to deploy your trained model. When done, you can see a list of your models using the models list command:

gcloud ai-platform models list

Send an online prediction request to your deployed model
You can now send prediction requests to your deployed model. The following command sends a prediction request using the test.json file included in the GitHub repository.

gcloud ai-platform predict \
--model $MODEL_NAME \
--version v1 \
--json-instances ../test.json

The response includes the probabilities of each label (>50K and <=50K) based on the data entry in test.json, thus indicating whether the predicted income is greater than or less than 50,000 dollars

CLASS_IDS  CLASSES  LOGISTIC               LOGITS                PROBABILITIES
[0]        [u'0']   [0.06142739951610565]  [-2.726504325866699]  [0.9385725855827332, 0.061427392065525055]
Where class 0 means income <= 50k and class 1 means income >50k.

Note: AI PLatform supports batch prediction too, but it's not included in this lab. See the documentation for more info.